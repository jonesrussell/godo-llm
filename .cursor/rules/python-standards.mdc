---
globs: *.py
description: Python coding standards and best practices for the LLM inference pipeline
---

# Python Coding Standards

## Code Style & Formatting

- **Formatter**: Use `black` with default settings
- **Linter**: Use `ruff` for fast linting
- **Type Checking**: Use `mypy` for static type analysis
- **Line Length**: 88 characters (black default)
- **Import Sorting**: Use `ruff` import sorting

## Async/Await Patterns

This project heavily uses async/await for FastAPI and Redis operations:

```python
# Preferred async patterns
async def generate_text(prompt: str) -> AsyncGenerator[str, None]:
    async with redis_client:
        # Redis operations
        cached_result = await redis_client.get(cache_key)
    
    # LLM inference with streaming
    async for token in model.generate(prompt):
        yield token
```

## Error Handling

- Use FastAPI's `HTTPException` for API errors
- Implement graceful fallbacks for model loading
- Log errors with structured logging
- Use context managers for resource cleanup

## Configuration Management

- Use Pydantic models for configuration validation
- Environment variables via `.env` files
- Type hints for all configuration classes

## Performance Considerations

- Use async Redis operations (`redis.asyncio`)
- Implement caching for repeated requests
- Stream responses for better UX
- Monitor GPU memory usage

## Testing Patterns

- Use `pytest` with `pytest-asyncio`
- Test both sync and async functions
- Mock external dependencies (Redis, model loading)
- Include integration tests for full API flow

## Dependencies

- Pin major versions in [requirements.txt](mdc:requirements.txt)
- Use version ranges for minor updates
- Separate dev dependencies from production
- Include CUDA-specific packages for GPU support