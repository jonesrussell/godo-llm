---
alwaysApply: true
description: Project structure and architecture guide for LLM inference pipeline
---

# Project Structure Guide

This is a **Local LLM Inference Pipeline** optimized for RTX 4060 with 8GB VRAM, featuring FastAPI streaming, Redis caching, and Vue.js frontend.

## Core Architecture

The main entry point is [main.py](mdc:main.py), which provides:
- FastAPI web service with streaming support
- Llama-2-7b model inference via llama-cpp-python
- Redis caching for performance optimization
- Server-Sent Events for real-time token streaming

## Key Files and Their Roles

### Core Application
- **[main.py](mdc:main.py)** - Main FastAPI application with LLM inference endpoints
- **[test_api.py](mdc:test_api.py)** - Comprehensive testing suite for API endpoints
- **[requirements.txt](mdc:requirements.txt)** - Python dependencies with CUDA support

### Setup and Configuration
- **[setup_model.py](mdc:setup_model.py)** - Automated model download and quantization setup
- **[setup_env.py](mdc:setup_env.py)** - Environment configuration management
- **[setup_environment.sh](mdc:setup_environment.sh)** - System dependencies setup script
- **[env.example](mdc:env.example)** - Environment variables template

### Deployment
- **[docker-compose.yml](mdc:docker-compose.yml)** - Multi-service setup with Redis and LLM API
- **[Dockerfile](mdc:Dockerfile)** - Container configuration for LLM service

### Frontend
- **[frontend/index.html](mdc:frontend/index.html)** - Vue.js web interface for testing

## Directory Structure
```
llm-godo/
â”œâ”€â”€ main.py                 # ğŸš€ Core API application
â”œâ”€â”€ test_api.py            # ğŸ§ª Comprehensive testing suite
â”œâ”€â”€ setup_model.py         # ğŸ“¥ Model setup (with automatic fallbacks)
â”œâ”€â”€ setup_env.py           # ğŸ”§ Environment configuration
â”œâ”€â”€ setup_environment.sh   # ğŸ› ï¸ System dependencies setup
â”œâ”€â”€ requirements.txt       # ğŸ“¦ Python dependencies
â”œâ”€â”€ env.example           # ğŸ“ Environment template
â”œâ”€â”€ Dockerfile            # ğŸ³ Container setup
â”œâ”€â”€ docker-compose.yml    # ğŸ³ Multi-service setup
â”œâ”€â”€ README.md             # ğŸ“– Documentation
â””â”€â”€ frontend/             # ğŸ¨ Web interface
    â””â”€â”€ index.html
```

## Model Configuration
- **Primary Model**: Llama-2-7b with Q4_K_M quantization
- **Fallback Models**: Pre-converted GGUF and TinyLlama for testing
- **GPU Optimization**: Configured for RTX 4060 (8GB VRAM) with 20 GPU layers
- **Performance**: 15-20 tokens/second, 50-80ms latency per token

## Service Architecture
- **FastAPI Service**: Handles HTTP requests and streaming responses
- **Redis Service**: Optional caching layer for improved performance
- **Model Service**: llama-cpp-python for efficient local inference
- **Frontend Service**: Vue.js interface for testing and interaction