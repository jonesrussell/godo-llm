---
description: Environment configuration and setup guidelines for LLM inference pipeline
---

# Environment Configuration Guide

## Environment Setup Process

### Automated Setup
Use the provided setup scripts for consistent environment configuration:

1. **Environment Variables**: Run [setup_env.py](mdc:setup_env.py) to create `.env` file
2. **System Dependencies**: Run [setup_environment.sh](mdc:setup_environment.sh) for system setup
3. **Model Setup**: Run [setup_model.py](mdc:setup_model.py) for model download and quantization

### Manual Setup
If automated setup fails, follow the manual process documented in [README.md](mdc:README.md).

## Environment Variables

### Required Configuration
All environment variables are defined in [env.example](mdc:env.example):

```bash
# Hugging Face Configuration (Required for model download)
HUGGINGFACE_HUB_TOKEN=your_token_here

# Model Configuration
MODEL_PATH=models/llama2-7b-q4.gguf
GPU_LAYERS=20
MODEL_CONTEXT_SIZE=2048
MODEL_BATCH_SIZE=512
```

### API Configuration
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
```

### Redis Configuration (Optional)
```bash
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_URL=redis://localhost:6379/0
```

### Performance Configuration
```bash
# Performance Configuration
MAX_TOKENS_DEFAULT=256
TEMPERATURE_DEFAULT=0.7
TOP_P_DEFAULT=0.9
TOP_K_DEFAULT=40
```

## Model Configuration

### GPU Optimization for RTX 4060
Optimize settings for RTX 4060 (8GB VRAM):

```bash
# GPU Configuration
GPU_LAYERS=20          # Use 20 GPU layers for optimal performance
MODEL_CONTEXT_SIZE=2048 # Context window size
MODEL_BATCH_SIZE=512   # Batch size for processing
```

### Model Paths
Configure model paths based on setup:

```bash
# Primary model (after running setup_model.py)
MODEL_PATH=models/llama2-7b-q4.gguf

# Fallback models
MODEL_PATH=models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf  # Test model
MODEL_PATH=models/llama2-7b-chat-hf/                     # Hugging Face format
```

## Development vs Production

### Development Configuration
```bash
# Development settings
DEBUG=true
RELOAD=true
LOG_LEVEL=DEBUG
```

### Production Configuration
```bash
# Production settings
DEBUG=false
RELOAD=false
LOG_LEVEL=INFO
API_WORKERS=4  # Adjust based on CPU cores
```

## Environment Validation

### Startup Checks
The application performs these validation checks:

1. **Model File Existence**: Verify model file exists at specified path
2. **GPU Availability**: Check CUDA installation and GPU memory
3. **Redis Connection**: Test Redis connectivity (optional)
4. **Environment Variables**: Validate required variables are set

### Error Handling
```python
# Model validation
if not os.path.exists(model_path):
    raise FileNotFoundError(f"Model not found at {model_path}. Please run setup_model.py first.")

# GPU validation
n_gpu_layers = int(os.getenv("GPU_LAYERS", "20"))
if n_gpu_layers > 0:
    # Check GPU availability
    try:
        import torch
        assert torch.cuda.is_available()
    except ImportError:
        print("⚠️  CUDA not available, falling back to CPU")
```

## Docker Environment

### Docker Compose Configuration
Environment variables in [docker-compose.yml](mdc:docker-compose.yml):

```yaml
environment:
  - MODEL_PATH=models/llama2-7b-q4.gguf
  - GPU_LAYERS=20
  - REDIS_URL=redis://redis:6379/0
```

### Docker Environment Variables
Override environment variables for Docker deployment:

```bash
# Docker-specific overrides
export MODEL_PATH=/app/models/llama2-7b-q4.gguf
export REDIS_URL=redis://redis:6379/0
export API_HOST=0.0.0.0
```

## Troubleshooting

### Common Environment Issues

**Missing Hugging Face Token:**
```bash
# Set token in .env file
echo "HUGGINGFACE_HUB_TOKEN=your_token_here" >> .env
```

**CUDA Not Available:**
```bash
# Check CUDA installation
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"
```

**Redis Connection Failed:**
```bash
# Start Redis service
redis-server
# Or use Docker
docker run -d -p 6379:6379 redis:7-alpine
```

**Model Not Found:**
```bash
# Run model setup
python setup_model.py
# Check model path
ls -la models/llama2-7b-q4.gguf
```

### Performance Tuning

**Memory Issues:**
```bash
# Reduce GPU layers
export GPU_LAYERS="15"

# Reduce context size
export MODEL_CONTEXT_SIZE="1024"
```

**Slow Performance:**
```bash
# Increase batch size
export MODEL_BATCH_SIZE="1024"

# Optimize GPU layers
export GPU_LAYERS="25"
```

## Security Considerations

### Environment File Security
- Never commit `.env` files to version control
- Use `.env.example` as template
- Rotate API keys regularly
- Use environment-specific configurations

### Production Security
```bash
# Production environment variables
export DEBUG=false
export LOG_LEVEL=WARNING
export API_HOST=127.0.0.1  # Bind to localhost only
```

## Monitoring and Logging

### Log Configuration
```bash
# Logging settings
LOG_LEVEL=INFO
LOG_FILE=logs/api.log
```

### Health Monitoring
Use built-in health endpoints:
- `GET /health` - Service health status
- `GET /models/info` - Model configuration info

### Performance Monitoring
Monitor these metrics:
- Token generation rate
- Response latency
- Memory usage
- Cache hit rate
- Error rates