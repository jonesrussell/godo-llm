{
  "name": "godo-llm",
  "version": "1.0.0",
  "description": "A complete local inference solution for Llama-2-7b on RTX 4060 with 8GB VRAM. Features FastAPI streaming, Redis caching, and a beautiful Vue.js frontend.",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview",
    "serve": "vite preview --port 3000",
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/jonesrussell/godo-llm.git"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "type": "commonjs",
  "bugs": {
    "url": "https://github.com/jonesrussell/godo-llm/issues"
  },
  "homepage": "https://github.com/jonesrussell/godo-llm#readme",
  "dependencies": {
    "@vitejs/plugin-vue": "^6.0.1",
    "@vueuse/core": "^13.9.0",
    "axios": "^1.12.2",
    "vite": "^7.1.6",
    "vue": "^3.5.21"
  }
}
