# =============================================================================
# LLM API Configuration
# =============================================================================

# Model Provider Configuration
DEFAULT_PROVIDER=huggingface  # Options: local, huggingface

# Hugging Face Configuration (add these to your existing HF section)
HUGGINGFACE_MODEL_NAME=microsoft/DialoGPT-medium
HUGGINGFACE_CACHE_DIR=./hf_cache

# Cache Configuration
CACHE_TTL=3600  # Cache time-to-live in seconds (1 hour)

# Streaming Configuration
STREAM_DELAY=0.01  # Delay between tokens in streaming mode

# Hugging Face Configuration
# HUGGINGFACE_HUB_TOKEN=your_token_here

# Model Configuration
MODELS_DIR=models
MODEL_PATH=models/llama2-7b-q4.gguf
MODEL_CONTEXT_SIZE=2048
MODEL_BATCH_SIZE=512
GPU_LAYERS=20
GPU_COUNT=1

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_URL=redis://localhost:6379/0

# Performance Configuration
MAX_TOKENS_DEFAULT=256
TEMPERATURE_DEFAULT=0.7
TOP_P_DEFAULT=0.9
TOP_K_DEFAULT=40

# Resource Limits (Docker Compose)
MEMORY_LIMIT=8G
MEMORY_RESERVATION=4G

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=logs/api.log

# Development Configuration
DEBUG=false
RELOAD=false

# Security Configuration
# Add any API keys or sensitive config here
# These will be loaded by the application and Docker Compose
