version: '3'

# Global variables
vars:
  PYTHON: python3
  PIP: pip
  VENV_DIR: venv
  MODELS_DIR: models
  API_HOST: localhost
  API_PORT: 8001
  API_BASE: "http://{{.API_HOST}}:{{.API_PORT}}"
tasks:
  # =============================================================================
  # HELP & INFO
  # =============================================================================
  help:
    desc: "Show available tasks"
    cmds:
      - task --list

  info:
    desc: "Show project information and status"
    cmds:
      - echo "ğŸš€ Local LLM Inference Pipeline"
      - echo "================================"
      - echo ""
      - echo "ğŸ“ Project Structure:"
      - echo "  main.py          - Core FastAPI application"
      - echo "  test_api.py      - Comprehensive testing suite"
      - echo "  setup_model.py   - Model download and setup"
      - echo "  setup_env.py     - Environment configuration"
      - echo ""
      - echo "ğŸ³ Docker Support:"
      - echo "  docker/          - Docker configuration files"
      - echo ""
      - echo "ğŸ¯ Quick Commands:"
      - echo "  task setup       - Complete environment setup"
      - echo "  task start       - Start the API server"
      - echo "  task test        - Run comprehensive tests"
      - echo "  task docker:up   - Start with Docker Compose"

  # =============================================================================
  # ENVIRONMENT SETUP
  # =============================================================================
  setup:
    desc: "Complete environment setup (dependencies + environment)"
    deps: [setup:deps]
    cmds:
      - task setup:env
      - echo "âœ… Environment setup complete!"
      - echo ""
      - echo "Next steps:"
      - echo "1. task model:download  - Download Llama-2-7b model"
      - echo "2. task start           - Start the API server"
      - echo "3. task test            - Run tests"

  "setup:deps":
    desc: "Install system dependencies"
    cmds:
      - echo "ğŸ”§ Installing system dependencies..."
      - |
        if command -v apt-get >/dev/null 2>&1; then
          echo "ğŸ“¦ Installing Ubuntu/Debian dependencies..."
          sudo apt-get update
          sudo apt-get install -y build-essential cmake gcc-12 g++-12 git wget curl python3-dev python3-pip python3-venv redis-server
          echo "ğŸ”— Setting up compiler alternatives..."
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 100
          sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-12 100
        elif command -v yum >/dev/null 2>&1; then
          echo "ğŸ“¦ Installing RHEL/CentOS dependencies..."
          sudo yum groupinstall -y "Development Tools"
          sudo yum install -y cmake git wget curl python3-devel python3-pip redis
        else
          echo "âš ï¸  Please install dependencies manually:"
          echo "   - build-essential (gcc, g++, make)"
          echo "   - cmake"
          echo "   - python3-dev"
          echo "   - redis-server"
        fi

  "setup:env":
    desc: "Create virtual environment and install Python dependencies"
    cmds:
      - echo "ğŸ Setting up Python environment..."
      - echo "ğŸ§¹ Cleaning any existing virtual environment..."
      - rm -rf {{.VENV_DIR}}
      - "{{.PYTHON}} -m venv {{.VENV_DIR}}"
      - echo "ğŸ“¦ Installing Python dependencies..."
      - "{{.VENV_DIR}}/bin/{{.PIP}} install --upgrade pip"
      - echo "ğŸ”§ Setting compiler environment variables..."
      - "CC=gcc-12 CXX=g++-12 {{.VENV_DIR}}/bin/{{.PIP}} install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      - "CC=gcc-12 CXX=g++-12 {{.VENV_DIR}}/bin/{{.PIP}} install -r requirements.txt"

  "setup:validate":
    desc: "Validate environment setup"
    cmds:
      - echo "ğŸ” Validating environment..."
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} -c 'import torch; print(f\"PyTorch: {torch.__version__}\"); print(f\"CUDA: {torch.cuda.is_available()}\")'"
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} -c 'import llama_cpp; print(f\"llama-cpp-python: {llama_cpp.__version__}\")'"
      - echo "âœ… Environment validation complete!"

  # =============================================================================
  # MODEL MANAGEMENT
  # =============================================================================
  "model:download":
    desc: "Download and setup Llama-2-7b model"
    cmds:
      - echo "ğŸ“¥ Downloading Llama-2-7b model..."
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} setup_model.py"

  "model:download:gguf":
    desc: "Download a pre-converted GGUF model for quick testing"
    cmds:
      - echo "ğŸ“¥ Downloading pre-converted GGUF model..."
      - mkdir -p {{.MODELS_DIR}}
      - |
        cd {{.MODELS_DIR}} && \
        wget -O tinyllama-1.1b-chat.gguf \
        "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
      - echo "âœ… GGUF model downloaded successfully!"

  "model:check":
    desc: "Check for existing models"
    cmds:
      - echo "ğŸ” Checking for existing models..."
      - |
        if [ -f "{{.MODELS_DIR}}/llama2-7b-q4.gguf" ]; then
          echo "âœ… Found Q4_K_M quantized model"
        elif [ -f "{{.MODELS_DIR}}/llama2-7b-chat.gguf" ]; then
          echo "âœ… Found GGUF model"
        elif [ -d "{{.MODELS_DIR}}/llama2-7b" ]; then
          echo "âœ… Found HuggingFace model"
        else
          echo "âŒ No models found - run 'task model:download'"
        fi

  "model:clean":
    desc: "Clean model files"
    cmds:
      - echo "ğŸ§¹ Cleaning model files..."
      - rm -rf {{.MODELS_DIR}}/llama2-7b*
      - rm -rf {{.MODELS_DIR}}/tinyllama*
      - echo "âœ… Model files cleaned"

  # =============================================================================
  # DEVELOPMENT
  # =============================================================================
  start:
    desc: "Start the API server"
    cmds:
      - echo "ğŸš€ Starting LLM API server..."
      - echo "ğŸ›‘ Stopping any existing API processes..."
      - pkill -f "python.*main.py" || true
      - echo "ğŸ›‘ Freeing up port 8001..."
      - lsof -ti:8001 | xargs kill -9 2>/dev/null || true
      - sleep 3
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} main.py"

  stop:
    desc: "Stop the API server"
    cmds:
      - echo "ğŸ›‘ Stopping LLM API server..."
      - pkill -f "python.*main.py" || echo "No API process found"
      - echo "ğŸ›‘ Freeing up port 8001..."
      - lsof -ti:8001 | xargs kill -9 2>/dev/null || echo "Port 8001 is free"
      - echo "âœ… API server stopped"

  "start:redis":
    desc: "Start Redis server"
    cmds:
      - echo "ğŸ—„ï¸  Starting Redis server..."
      - redis-server --daemonize yes
      - echo "âœ… Redis server started"

  "start:frontend":
    desc: "Start frontend development server"
    cmds:
      - echo "ğŸ¨ Starting frontend server..."
      - cd frontend && {{.PYTHON}} -m http.server 8080

  dev:
    desc: "Start development environment (Redis + API + Frontend)"
    deps: [start:redis]
    cmds:
      - "echo ğŸ”§ Starting development environment..."
      - "echo Redis: http://localhost:6379"
      - "echo API: http://localhost:8000" 
      - "echo Frontend: http://localhost:8080"
      - "echo"
      - "echo Starting API server..."
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} main.py"

  # =============================================================================
  # TESTING
  # =============================================================================
  test:
    desc: "Run comprehensive API tests"
    cmds:
      - echo "ğŸ§ª Running comprehensive tests..."
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} test_api.py"

  "test:health":
    desc: "Test health endpoint"
    cmds:
      - echo "ğŸ” Testing health endpoint..."
      - curl -f {{.API_BASE}}/health || echo "âŒ Health check failed"

  "test:model":
    desc: "Test model info endpoint"
    cmds:
      - echo "ğŸ” Testing model info..."
      - curl -f {{.API_BASE}}/models/info || echo "âŒ Model info failed"

  "test:generate":
    desc: "Test text generation"
    cmds:
      - echo "ğŸ” Testing text generation..."
      - |
        curl -X POST "{{.API_BASE}}/generate" \
          -H "Content-Type: application/json" \
          -d '{"prompt": "Hello, how are you?", "max_tokens": 20, "stream": false}' \
          -w "Time: %{time_total}s\n" || echo "âŒ Generation test failed"

  "test:stream":
    desc: "Test streaming generation"
    cmds:
      - echo "ğŸ” Testing streaming generation..."
      - |
        curl -X POST "{{.API_BASE}}/generate" \
          -H "Content-Type: application/json" \
          -d '{"prompt": "Tell me a story", "max_tokens": 50, "stream": true}' \
          --no-buffer || echo "âŒ Streaming test failed"

  "test:quick":
    desc: "Quick API test"
    cmds:
      - echo "ğŸ” Quick API test..."
      - curl -s {{.API_BASE}}/health | jq -r '.status // "ERROR"'
      - echo "ğŸš€ Testing generation..."
      - |
        curl -X POST "{{.API_BASE}}/generate" \
          -H "Content-Type: application/json" \
          -d '{"prompt": "Hello", "max_tokens": 10, "stream": false}' \
          -s | jq -r '.text // "ERROR"'

  # =============================================================================
  # MONITORING & DEBUGGING
  # =============================================================================
  logs:
    desc: "Show API logs"
    cmds:
      - echo "ğŸ“‹ Showing API logs..."
      - tail -f logs/api.log 2>/dev/null || echo "No log file found"

  "monitor:gpu":
    desc: "Monitor GPU usage"
    cmds:
      - echo "ğŸ–¥ï¸  Monitoring GPU usage..."
      - watch -n 1 nvidia-smi

  "monitor:redis":
    desc: "Monitor Redis status"
    cmds:
      - echo "ğŸ—„ï¸  Checking Redis status..."
      - redis-cli ping || echo "âŒ Redis not running"

  "monitor:api":
    desc: "Monitor API performance"
    cmds:
      - echo "ğŸ“Š Monitoring API performance..."
      - |
        while true; do
          echo "$(date): $(curl -s {{.API_BASE}}/health | jq -r '.status // "ERROR"')"
          sleep 5
        done

  # =============================================================================
  # DOCKER OPERATIONS
  # =============================================================================
  "docker:build":
    desc: "Build Docker images"
    cmds:
      - echo "ğŸ³ Building Docker images..."
      - docker-compose -f docker/docker-compose.yml build

  "docker:up":
    desc: "Start services with Docker Compose"
    cmds:
      - echo "ğŸ³ Starting Docker services..."
      - docker-compose -f docker/docker-compose.yml up --build

  "docker:up:dev":
    desc: "Start development environment with Docker"
    cmds:
      - echo "ğŸ³ Starting development environment..."
      - docker-compose -f docker/docker-compose.yml -f docker/docker-compose.dev.yml up --build

  "docker:up:prod":
    desc: "Start production environment with Docker"
    cmds:
      - echo "ğŸ³ Starting production environment..."
      - docker-compose -f docker/docker-compose.yml -f docker/docker-compose.prod.yml up --build

  "docker:down":
    desc: "Stop Docker services"
    cmds:
      - echo "ğŸ³ Stopping Docker services..."
      - docker-compose -f docker/docker-compose.yml down

  "docker:logs":
    desc: "Show Docker service logs"
    cmds:
      - echo "ğŸ“‹ Showing Docker logs..."
      - docker-compose -f docker/docker-compose.yml logs -f

  "docker:clean":
    desc: "Clean Docker resources"
    cmds:
      - echo "ğŸ§¹ Cleaning Docker resources..."
      - docker-compose -f docker/docker-compose.yml down -v
      - docker system prune -f

  # =============================================================================
  # MAINTENANCE
  # =============================================================================
  clean:
    desc: "Clean project files"
    cmds:
      - echo "ğŸ§¹ Cleaning project files..."
      - rm -rf __pycache__/
      - rm -rf *.pyc
      - rm -rf .pytest_cache/
      - rm -rf logs/*.log
      - echo "âœ… Project cleaned"

  "clean:all":
    desc: "Clean all files including models and Docker"
    deps: [model:clean, docker:clean]
    cmds:
      - echo "ğŸ§¹ Deep cleaning project..."
      - rm -rf {{.VENV_DIR}}/
      - rm -rf logs/
      - echo "âœ… Deep clean complete"

  update:
    desc: "Update dependencies"
    cmds:
      - echo "â¬†ï¸  Updating dependencies..."
      - "{{.VENV_DIR}}/bin/{{.PIP}} install --upgrade pip"
      - "{{.VENV_DIR}}/bin/{{.PIP}} install -r requirements.txt --upgrade"

  # =============================================================================
  # QUICK ACTIONS
  # =============================================================================
  quickstart:
    desc: "Quick start for new users"
    cmds:
      - echo "ğŸš€ Quick Start Guide"
      - echo "==================="
      - echo ""
      - echo "1. Setting up environment..."
      - task setup
      - echo ""
      - echo "2. Downloading model..."
      - task model:download:gguf
      - echo ""
      - echo "3. Starting Redis..."
      - task start:redis
      - echo ""
      - echo "4. Testing setup..."
      - task test
      - echo ""
      - echo "âœ… Quick start complete!"
      - echo ""
      - echo "ğŸš€ Ready to start the API server!"
      - echo "ğŸ“¡ To start - task start"
      - echo "ğŸ“– API Docs - http://localhost:8001/docs"
      - echo "ğŸ›‘ To stop - task stop"

  status:
    desc: "Check system status"
    cmds:
      - echo "ğŸ“Š System Status"
      - echo "==============="
      - echo ""
      - echo "ğŸ Python Environment:"
      - "{{.VENV_DIR}}/bin/{{.PYTHON}} --version"
      - echo ""
      - echo "ğŸ—„ï¸  Redis Status:"
      - redis-cli ping 2>/dev/null && echo "âœ… Redis running" || echo "âŒ Redis not running"
      - echo ""
      - echo "ğŸ–¥ï¸  GPU Status:"
      - nvidia-smi --query-gpu=name,memory.used,memory.total --format=csv,noheader,nounits 2>/dev/null || echo "âŒ GPU not available"
      - echo ""
      - echo "ğŸ“ Model Status:"
      - task model:check
      - echo ""
      - echo "ğŸŒ API Status:"
      - curl -s {{.API_BASE}}/health 2>/dev/null | jq -r '.status // "Not running"' || echo "âŒ API not running"
