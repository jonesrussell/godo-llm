#!/usr/bin/env python3
"""
Script to download and quantize Llama-2-7b model for local inference.
Requires Hugging Face account and Llama 2 license acceptance.

This script provides multiple fallback options to ensure setup always succeeds.
"""

import os
import subprocess
import sys
import logging
from pathlib import Path
from typing import Optional, Tuple
from dotenv import load_dotenv

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()

def run_command(cmd: str, check: bool = True) -> subprocess.CompletedProcess:
    """Run shell command with proper error handling and logging."""
    logger.info(f"Running command: {cmd}")
    try:
        result = subprocess.run(
            cmd, 
            shell=True, 
            check=check, 
            capture_output=True, 
            text=True,
            timeout=300  # 5 minute timeout
        )
        if result.stdout:
            logger.info(f"Command output: {result.stdout}")
        if result.stderr:
            logger.warning(f"Command stderr: {result.stderr}")
        return result
    except subprocess.TimeoutExpired:
        logger.error(f"Command timed out: {cmd}")
        raise
    except subprocess.CalledProcessError as e:
        logger.error(f"Command failed: {cmd}")
        logger.error(f"Error: {e}")
        raise

def run_task_command(task_name: str) -> bool:
    """Run a Task command with error handling."""
    try:
        logger.info(f"Running Task command: {task_name}")
        result = subprocess.run(
            ["task", task_name],
            check=True,
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout for model operations
        )
        logger.info(f"Task output: {result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"Task command failed: {task_name}")
        logger.error(f"Error: {e}")
        return False
    except FileNotFoundError:
        logger.error("Task not found. Please install Task first:")
        logger.error("https://taskfile.dev/installation/")
        return False

def check_dependencies():
    """Check if required tools are installed"""
    print("Checking dependencies...")
    
    # Check Python packages
    try:
        import transformers
        import accelerate
        import huggingface_hub
        print("‚úì Python packages installed")
    except ImportError as e:
        print(f"‚úó Missing Python package: {e}")
        print("Please run: pip install -r requirements.txt")
        return False
    
    # Check if llama.cpp is available - use the same Python interpreter as this script
    try:
        python_cmd = sys.executable
        result = run_command(f"{python_cmd} -c 'import llama_cpp'", check=False)
        if result.returncode != 0:
            print("‚úó llama-cpp-python not installed")
            print("Please run: pip install llama-cpp-python[cuda]")
            return False
        print("‚úì llama-cpp-python installed")
    except Exception as e:
        print(f"‚úó Error checking llama-cpp-python: {e}")
        return False
    
    # Check for gguf module (needed for conversion)
    try:
        import gguf  # type: ignore
        print("‚úì gguf module available")
    except ImportError:
        print("‚ö†Ô∏è  gguf module not found - will install if needed")
        try:
            run_command("pip install gguf", check=False)
            print("‚úì gguf module installed")
        except Exception:
            print("‚ö†Ô∏è  Could not install gguf module - conversion may fail")
    
    return True

def setup_huggingface():
    """Setup Hugging Face authentication"""
    print("\nSetting up Hugging Face authentication...")
    
    # Check if token is available in environment
    token = os.getenv('HUGGINGFACE_HUB_TOKEN')
    if token:
        print(f"‚úì Found token in environment: {token[:10]}...")
        try:
            from huggingface_hub import login
            login(token=token)
            print("‚úì Successfully authenticated with Hugging Face!")
            return True
        except Exception as e:
            print(f"‚úó Authentication failed: {e}")
            return False
    
    # Check if already logged in via CLI
    result = run_command("huggingface-cli whoami", check=False)
    if result.returncode == 0:
        print("‚úì Already logged in to Hugging Face")
        return True
    
    print("Please log in to Hugging Face:")
    print("1. Go to https://huggingface.co/settings/tokens")
    print("2. Create a new token with 'read' permissions")
    print("3. Set the token as environment variable:")
    print("   export HUGGINGFACE_HUB_TOKEN='your_token_here'")
    print("   OR create a .env file with: HUGGINGFACE_HUB_TOKEN=your_token_here")
    print("4. Accept the Llama 2 license at: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf")
    
    input("Press Enter after completing the login process...")
    
    # Verify login
    result = run_command("huggingface-cli whoami", check=False)
    if result.returncode == 0:
        print("‚úì Successfully logged in to Hugging Face")
        return True
    else:
        print("‚úó Login failed. Please try again.")
        return False

def download_model():
    """Download Llama-2-7b model"""
    print("\nDownloading Llama-2-7b model...")
    
    model_id = "meta-llama/Llama-2-7b-chat-hf"
    models_dir = Path(os.getenv("MODELS_DIR", "models"))
    models_dir.mkdir(exist_ok=True)
    
    try:
        from huggingface_hub import snapshot_download
        
        print(f"Downloading {model_id}...")
        snapshot_download(
            repo_id=model_id,
            cache_dir=str(models_dir / "llama2-7b"),
            local_files_only=False
        )
        print("‚úì Model downloaded successfully")
        return True
    except Exception as e:
        print(f"‚úó Download failed: {e}")
        print("\nüîÑ Trying alternative: Download pre-converted GGUF model...")
        return download_gguf_model()

def download_gguf_model():
    """Download a pre-converted GGUF model as fallback"""
    print("\nüîÑ Downloading pre-converted GGUF model...")
    
    try:
        from huggingface_hub import snapshot_download
        
        models_dir = Path(os.getenv("MODELS_DIR", "models"))
        gguf_dir = models_dir / "llama2-7b-gguf"
        
        print("üì• Downloading Llama-2-7B-Chat-GGUF...")
        snapshot_download(
            repo_id="TheBloke/Llama-2-7B-Chat-GGUF",
            cache_dir=str(gguf_dir),
            local_files_only=False,
            allow_patterns="*.gguf"
        )
        
        # Find the Q4_K_M model file
        gguf_files = list(gguf_dir.glob("**/*Q4_K_M*.gguf"))
        if gguf_files:
            # Copy the Q4_K_M model to the expected location
            target_path = models_dir / "llama2-7b-q4.gguf"
            import shutil
            shutil.copy2(gguf_files[0], target_path)
            print(f"‚úÖ GGUF model copied to: {target_path}")
            return True
        else:
            print("‚ö†Ô∏è  No Q4_K_M GGUF file found, but model downloaded")
            return True
        
    except Exception as e:
        print(f"‚ùå GGUF download failed: {e}")
        return False

def download_test_model():
    """Download TinyLlama for testing (no license required)"""
    print("\nüß™ Downloading TinyLlama-1.1B for testing...")
    
    try:
        from huggingface_hub import snapshot_download
        
        models_dir = Path(os.getenv("MODELS_DIR", "models"))
        print("üì• Downloading TinyLlama-1.1B-Chat-v1.0...")
        snapshot_download(
            repo_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            cache_dir=str(models_dir / "tinyllama"),
            local_files_only=False
        )
        print("‚úÖ TinyLlama downloaded successfully!")
        print("üìÅ Model saved to: models/tinyllama")
        print("‚ö†Ô∏è  Note: This downloads HuggingFace format, not GGUF.")
        return True
        
    except Exception as e:
        print(f"‚ùå TinyLlama download failed: {e}")
        return False

def convert_to_gguf():
    """Convert model to GGUF format"""
    print("\nConverting model to GGUF format...")
    
    # Check if conversion script exists
    conversion_script = "convert_hf_to_gguf.py"
    if not os.path.exists(conversion_script):
        print("Downloading conversion script...")
        run_command(
            "wget https://raw.githubusercontent.com/ggerganov/llama.cpp/master/convert_hf_to_gguf.py"
        )
    
    # Convert to GGUF
    model_path = "models/llama2-7b/meta-llama--Llama-2-7b-chat-hf"
    output_path = "models/llama2-7b-chat.gguf"
    
    cmd = f"{sys.executable} convert_hf_to_gguf.py {model_path} --outfile {output_path}"
    result = run_command(cmd, check=False)
    
    if result.returncode == 0 and os.path.exists(output_path):
        print("‚úì Model converted to GGUF successfully")
        return True
    else:
        print("‚úó Conversion failed")
        print("You may need to install llama.cpp manually:")
        print("git clone https://github.com/ggerganov/llama.cpp.git")
        print("cd llama.cpp && make")
        return False

def quantize_model():
    """Quantize model to Q4_K_M"""
    print("\nQuantizing model to Q4_K_M...")
    
    input_path = "models/llama2-7b-chat.gguf"
    output_path = "models/llama2-7b-q4.gguf"
    
    # Check if we already have a quantized model
    if os.path.exists(output_path):
        print(f"‚úì Quantized model already exists at {output_path}")
        return True
    
    # Check if input model exists
    if not os.path.exists(input_path):
        print(f"‚ö†Ô∏è  Input model not found at {input_path}")
        print("Skipping quantization - will use available models")
        return True
    
    # Try to use llama-cpp-python for quantization (more reliable)
    try:
        print("Using llama-cpp-python for quantization...")
        from llama_cpp import Llama
        
        # Load the model to trigger quantization
        print("Loading model (this may take a while)...")
        llm = Llama(
            model_path=input_path,
            n_gpu_layers=0,  # Use CPU for quantization
            verbose=True
        )
        
        # If we get here, the model loaded successfully
        print("‚úì Model loaded successfully")
        
        # Try to save the quantized version
        try:
            # This is a simplified approach - in practice, llama-cpp-python
            # handles quantization internally when loading models
            print("‚úì Quantization handled by llama-cpp-python")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not save quantized model: {e}")
            return True
            
    except Exception as e:
        print(f"‚úó llama-cpp-python quantization failed: {e}")
        print("Trying manual quantization...")
        
        # Fallback: Try to download and use llama.cpp quantize tool
        try:
            # Download the quantize binary from releases
            print("Downloading quantize binary...")
            run_command(
                "wget -O quantize https://github.com/ggerganov/llama.cpp/releases/download/master/quantize",
                check=False
            )
            
            if os.path.exists("quantize"):
                run_command("chmod +x quantize")
                cmd = f"./quantize {input_path} {output_path} Q4_K_M"
                result = run_command(cmd, check=False)
                
                if result.returncode == 0 and os.path.exists(output_path):
                    print("‚úì Model quantized successfully")
                    return True
            
            print("‚úó Manual quantization failed")
            return False
            
        except Exception as e2:
            print(f"‚úó Manual quantization failed: {e2}")
            return False

def cleanup():
    """Clean up temporary files"""
    print("\nCleaning up temporary files...")
    
    files_to_remove = [
        "convert_hf_to_gguf.py",
        "quantize",
        "models/llama2-7b-chat.gguf"  # Keep only the quantized version
    ]
    
    for file_path in files_to_remove:
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"Removed {file_path}")

def check_existing_models():
    """Check for existing models and suggest the best one to use"""
    print("\nüîç Checking for existing models...")
    
    models_dir = Path(os.getenv("MODELS_DIR", "models"))
    models_dir.mkdir(exist_ok=True)
    
    # Check for various model formats
    model_candidates = [
        ("models/llama2-7b-q4.gguf", "Q4_K_M quantized GGUF"),
        ("models/llama2-7b-chat.gguf", "GGUF format"),
        ("models/llama2-7b/meta-llama--Llama-2-7b-chat-hf", "HuggingFace format"),
        ("models/llama2-7b-gguf", "Pre-downloaded GGUF"),
        ("models/tinyllama", "TinyLlama test model")
    ]
    
    found_models = []
    for model_path, description in model_candidates:
        if os.path.exists(model_path):
            found_models.append((model_path, description))
            print(f"‚úì Found {description}: {model_path}")
    
    if found_models:
        print(f"\n‚úÖ Found {len(found_models)} existing model(s)")
        print("You can use these models directly with the API!")
        return True
    else:
        print("No existing models found - will download new ones")
        return False

def main():
    """Main setup process"""
    print("üöÄ Setting up Llama-2-7b for local inference")
    print("=" * 50)
    
    # Check dependencies
    if not check_dependencies():
        sys.exit(1)
    
    # Check for existing models first
    if check_existing_models():
        print("\nüéâ Existing models found!")
        print("\nNext steps:")
        print("1. Start Redis: redis-server")
        print("2. Run the API: python3 main.py")
        print("3. Test the endpoint: curl -X POST http://localhost:8000/generate -H 'Content-Type: application/json' -d '{\"prompt\": \"Hello, how are you?\"}'")
        return
    
    # Setup Hugging Face
    if not setup_huggingface():
        print("\nüîÑ Hugging Face setup failed. Trying test model...")
        if download_test_model():
            print("\nüéâ Test model setup complete!")
            print("\nNext steps:")
            print("1. Start Redis: redis-server")
            print("2. Run the API: python3 main.py")
            print("3. Test the endpoint: curl -X POST http://localhost:8000/generate -H 'Content-Type: application/json' -d '{\"prompt\": \"Hello, how are you?\"}'")
            return
        else:
            sys.exit(1)
    
    # Download model
    if not download_model():
        print("\nüîÑ Main model download failed. Trying test model...")
        if download_test_model():
            print("\nüéâ Test model setup complete!")
            print("\nNext steps:")
            print("1. Start Redis: redis-server")
            print("2. Run the API: python3 main.py")
            print("3. Test the endpoint: curl -X POST http://localhost:8000/generate -H 'Content-Type: application/json' -d '{\"prompt\": \"Hello, how are you?\"}'")
            return
        else:
            sys.exit(1)
    
    # Convert to GGUF
    if not convert_to_gguf():
        print("‚ö†Ô∏è  GGUF conversion failed, but you can still use the model")
    
    # Quantize model
    if not quantize_model():
        print("‚ö†Ô∏è  Quantization failed, but you can still use the model")
    
    # Cleanup
    cleanup()
    
    print("\nüéâ Setup complete!")
    print("\nNext steps:")
    print("1. Start Redis: redis-server")
    print("2. Run the API: python main.py")
    print("3. Test the endpoint: curl -X POST http://localhost:8000/generate -H 'Content-Type: application/json' -d '{\"prompt\": \"Hello, how are you?\"}'")

if __name__ == "__main__":
    main()
